import os
import json
import logging
from datetime import datetime, date, timezone
from typing import List, Tuple, Optional, Dict
from core.config import Config
from llm.deepseek import DeepSeekClient

class AIReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨ï¼šè¯»å–è®¢é˜…åŸå§‹æ•°æ®ï¼Œç”ŸæˆAIæ€»ç»“æŠ¥å‘Š"""
    
    def __init__(self, config: Config, deepseek_api_key: Optional[str]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.raw_data_dir = config.get("subscription.raw_data_dir", "data/raw_subscription_data")
        self.report_output_dir = config.get("report.output_dir", "ai_reports")
        os.makedirs(self.report_output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–å¤§æ¨¡å‹å®¢æˆ·ç«¯
        self.deepseek_client = DeepSeekClient(
            api_key=deepseek_api_key,
            config=config
        ) if deepseek_api_key else None

    @staticmethod
    def _ensure_naive_datetime(dt: datetime) -> datetime:
        """ç¡®ä¿datetimeå¯¹è±¡ä¸åŒ…å«æ—¶åŒºä¿¡æ¯ï¼ˆè½¬ä¸ºoffset-naiveï¼‰"""
        if dt.tzinfo is not None and dt.tzinfo.utcoffset(dt) is not None:
            # è½¬æ¢ä¸ºUTCæ—¶é—´åç§»é™¤æ—¶åŒºä¿¡æ¯
            return dt.astimezone(timezone.utc).replace(tzinfo=None)
        return dt

    @staticmethod
    def _parse_iso_datetime(date_str: str) -> datetime:
        """è§£æISOæ ¼å¼çš„æ—¶é—´å­—ç¬¦ä¸²ï¼Œè¿”å›æ— æ—¶åŒºä¿¡æ¯çš„datetimeå¯¹è±¡"""
        dt = datetime.fromisoformat(date_str)
        return AIReportGenerator._ensure_naive_datetime(dt)

    def load_subscription_raw_data(self, 
                                  sub_id: Optional[int] = None,
                                  start_time: Optional[datetime] = None,
                                  end_time: Optional[datetime] = None) -> List[dict]:
        """
        åŠ è½½è®¢é˜…åŸå§‹æ•°æ®ï¼ˆæ”¯æŒæ—¶é—´èŒƒå›´ç­›é€‰ï¼‰
        
        Args:
            sub_id: å¯é€‰ï¼Œè®¢é˜…IDè¿‡æ»¤
            start_time: å¯é€‰ï¼Œå¼€å§‹æ—¶é—´è¿‡æ»¤
            end_time: å¯é€‰ï¼Œç»“æŸæ—¶é—´è¿‡æ»¤
            
        Returns:
            ç¬¦åˆæ¡ä»¶çš„åŸå§‹æ•°æ®åˆ—è¡¨
        """
        raw_data_list = []
        if not os.path.exists(self.raw_data_dir):
            return raw_data_list
        
        # å¤„ç†æŸ¥è¯¢æ—¶é—´ï¼ˆç¡®ä¿æ— æ—¶åŒºä¿¡æ¯ï¼‰
        query_start = self._ensure_naive_datetime(start_time) if start_time else None
        query_end = self._ensure_naive_datetime(end_time) if end_time else None
        
        # éå†æ‰€æœ‰åŸå§‹æ•°æ®æ–‡ä»¶
        for filename in os.listdir(self.raw_data_dir):
            if not filename.endswith("_raw.json"):
                continue
            file_path = os.path.join(self.raw_data_dir, filename)
            
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                
                # ç­›é€‰è®¢é˜…ID
                if sub_id is not None and data["subscription_id"] != sub_id:
                    continue
                
                # è§£ææ•°æ®æ—¶é—´èŒƒå›´ï¼ˆç¡®ä¿æ— æ—¶åŒºä¿¡æ¯ï¼‰
                try:
                    data_start = self._parse_iso_datetime(data["time_range"]["start"])
                    data_end = self._parse_iso_datetime(data["time_range"]["end"])
                except Exception as e:
                    self.logger.warning(f"è§£æ {filename} çš„æ—¶é—´èŒƒå›´å¤±è´¥: {str(e)}")
                    continue
                
                # ç­›é€‰æ—¶é—´èŒƒå›´
                if query_start and data_end < query_start:
                    continue
                if query_end and data_start > query_end:
                    continue
                
                raw_data_list.append(data)
            except Exception as e:
                self.logger.error(f"åŠ è½½åŸå§‹æ•°æ® {filename} å¤±è´¥: {str(e)}")
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
        return sorted(raw_data_list, key=lambda x: x["generated_at"], reverse=True)

    def generate_single_raw_report(self, raw_data: dict) -> Tuple[bool, str, Optional[str]]:
        """åŸºäºå•æ¡åŸå§‹æ•°æ®ç”ŸæˆAIæ€»ç»“æŠ¥å‘Š"""
        if not self.deepseek_client:
            return False, "æœªé…ç½®DeepSeek API Keyï¼Œæ— æ³•ç”ŸæˆAIæ€»ç»“", None
        
        try:
            # 1. æå–åŸå§‹æ•°æ®
            sub_id = raw_data["subscription_id"]
            repo_full_name = raw_data["repo_full_name"]
            time_range = raw_data["time_range"]
            repo_info = raw_data["data"]["repo_info"]
            releases = raw_data["data"]["releases"]
            prs = raw_data["data"]["pull_requests"]
            issues = raw_data["data"]["issues"]

            # 2. AIæ€»ç»“
            self.logger.info(f"ç”Ÿæˆè®¢é˜…ID {sub_id}ï¼ˆ{repo_full_name}ï¼‰çš„AIæ€»ç»“...")
            summaries = {
                "releases": self.deepseek_client.summarize_releases(releases),
                "issues_prs": self.deepseek_client.summarize_issues_prs(issues, prs)
            }

            # 3. ç”ŸæˆMarkdownå†…å®¹
            markdown_content = self._format_markdown(
                repo_info=repo_info,
                time_range=time_range,
                summaries=summaries,
                raw_data=raw_data["data"]
            )

            # 4. ä¿å­˜æŠ¥å‘Š
            start_date = self._parse_iso_datetime(time_range["start"]).strftime("%Y%m%d")
            safe_repo_name = repo_full_name.replace("/", "_")
            report_filename = f"{start_date}_sub{sub_id}_{safe_repo_name}_ai_report.md"
            report_path = os.path.join(self.report_output_dir, report_filename)

            with open(report_path, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            return True, f"æŠ¥å‘Šç”ŸæˆæˆåŠŸ", report_path
        except Exception as e:
            return False, f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"

    def generate_subscription_report(self, 
                                    sub_id: int,
                                    start_time: Optional[datetime] = None,
                                    end_time: Optional[datetime] = None) -> Tuple[bool, str, Optional[str]]:
        """ç”ŸæˆæŠ¥å‘Šï¼ˆæ”¯æŒæ—¶é—´èŒƒå›´ï¼Œé»˜è®¤æœ€æ–°æ•°æ®ï¼‰"""
        # åŠ è½½ç¬¦åˆæ¡ä»¶çš„åŸå§‹æ•°æ®
        raw_data_list = self.load_subscription_raw_data(
            sub_id=sub_id,
            start_time=start_time,
            end_time=end_time
        )
        if not raw_data_list:
            return False, f"æœªæ‰¾åˆ°è®¢é˜…ID {sub_id} çš„åŸå§‹æ•°æ®", None
        
        # æŒ‰æ—¶é—´èŒƒå›´ç”ŸæˆæŠ¥å‘Šï¼ˆå¤šä»½æ•°æ®åˆå¹¶ï¼‰
        if start_time and end_time:
            return self._generate_merged_report(raw_data_list, start_time, end_time)
        # å¦åˆ™ä½¿ç”¨æœ€æ–°çš„å•ä»½æ•°æ®
        return self.generate_single_raw_report(raw_data_list[0])

    def _generate_merged_report(self, raw_data_list: List[dict], start_time: datetime, end_time: datetime) -> Tuple[bool, str, Optional[str]]:
        """åˆå¹¶å¤šä¸ªåŸå§‹æ•°æ®ç”ŸæˆæŠ¥å‘Š"""
        if not self.deepseek_client:
            return False, "æœªé…ç½®DeepSeek API Key", None
        
        try:
            # åˆå¹¶æ•°æ®
            merged_data = {
                "releases": [],
                "pull_requests": [],
                "issues": []
            }
            repo_info = raw_data_list[0]["data"]["repo_info"]
            for data in raw_data_list:
                merged_data["releases"].extend(data["data"]["releases"])
                merged_data["pull_requests"].extend(data["data"]["pull_requests"])
                merged_data["issues"].extend(data["data"]["issues"])
            
            # å»é‡ï¼ˆæŒ‰IDï¼‰
            merged_data["releases"] = list({r["id"]: r for r in merged_data["releases"]}.values())
            merged_data["pull_requests"] = list({p["id"]: p for p in merged_data["pull_requests"]}.values())
            merged_data["issues"] = list({i["id"]: i for i in merged_data["issues"]}.values())

            # AIæ€»ç»“
            summaries = {
                "releases": self.deepseek_client.summarize_releases(merged_data["releases"]),
                "issues_prs": self.deepseek_client.summarize_issues_prs(
                    merged_data["issues"], 
                    merged_data["pull_requests"]
                )
            }

            # ç”ŸæˆæŠ¥å‘Š
            start_str = self._ensure_naive_datetime(start_time).strftime("%Y%m%d")
            end_str = self._ensure_naive_datetime(end_time).strftime("%Y%m%d")
            safe_repo_name = repo_info["full_name"].replace("/", "_")
            report_filename = f"{start_str}_to_{end_str}_sub{raw_data_list[0]['subscription_id']}_{safe_repo_name}_ai_report.md"
            report_path = os.path.join(self.report_output_dir, report_filename)

            markdown_content = self._format_markdown(
                repo_info=repo_info,
                time_range={
                    "start": start_time.isoformat(),
                    "end": end_time.isoformat()
                },
                summaries=summaries,
                raw_data=merged_data
            )

            with open(report_path, "w", encoding="utf-8") as f:
                f.write(markdown_content)
            return True, f"åˆå¹¶æŠ¥å‘Šç”ŸæˆæˆåŠŸ", report_path
        except Exception as e:
            return False, f"åˆå¹¶æŠ¥å‘Šå¤±è´¥: {str(e)}", None

    def generate_all_reports(self) -> Tuple[int, int, List[str]]:
        """ç”Ÿæˆæ‰€æœ‰æœªå¤„ç†çš„åŸå§‹æ•°æ®æŠ¥å‘Šï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰"""
        raw_data_list = self.load_subscription_raw_data()
        if not raw_data_list:
            return 0, 0, []
        
        success_count = 0
        report_paths = []
        for raw_data in raw_data_list:
            # æ£€æŸ¥æŠ¥å‘Šæ˜¯å¦å·²å­˜åœ¨
            start_date = self._parse_iso_datetime(raw_data["time_range"]["start"]).strftime("%Y%m%d")
            sub_id = raw_data["subscription_id"]
            safe_repo_name = raw_data["repo_full_name"].replace("/", "_")
            report_filename = f"{start_date}_sub{sub_id}_{safe_repo_name}_ai_report.md"
            report_path = os.path.join(self.report_output_dir, report_filename)
            
            if os.path.exists(report_path):
                self.logger.info(f"æŠ¥å‘Šå·²å­˜åœ¨ï¼Œè·³è¿‡ï¼š{report_filename}")
                continue
            
            # ç”ŸæˆæŠ¥å‘Š
            success, msg, path = self.generate_single_raw_report(raw_data)
            if success and path:
                success_count += 1
                report_paths.append(path)
        
        return success_count, len(raw_data_list), report_paths

    def _format_markdown(self, repo_info: dict, time_range: dict, summaries: dict, raw_data: dict) -> str:
        """æ ¼å¼åŒ–MarkdownæŠ¥å‘Šå†…å®¹"""
        generated_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        start_time = self._parse_iso_datetime(time_range["start"]).strftime("%Y-%m-%d %H:%M")
        end_time = self._parse_iso_datetime(time_range["end"]).strftime("%Y-%m-%d %H:%M")

        # æŠ¥å‘Šå¤´éƒ¨
        md = [
            f"# ğŸ¤– GitHubè®¢é˜…AIæ€»ç»“æŠ¥å‘Š",
            f"**ä»“åº“**: {repo_info.get('full_name', 'æœªçŸ¥')} [{repo_info.get('html_url', '')}]({repo_info.get('html_url', '')})",
            f"**æ—¶é—´èŒƒå›´**: {start_time} ~ {end_time}",
            f"**ç”Ÿæˆæ—¶é—´**: {generated_at}",
            "",
            "## ğŸ“Š ä»“åº“åŸºæœ¬ä¿¡æ¯",
            f"- åç§°: {repo_info.get('name', 'æœªçŸ¥')}",
            f"- æè¿°: {repo_info.get('description', 'æ— æè¿°')}",
            f"- æ˜Ÿçº§: {repo_info.get('stargazers_count', 0)} â­",
            f"- åˆ†æ”¯: {repo_info.get('forks_count', 0)} ğŸ´",
            "",
            "---",
            "## ğŸ“ AIæ™ºèƒ½æ€»ç»“",
        ]

        # å„æ¨¡å—æ€»ç»“
        md.extend([
            "### ğŸ”– å‘å¸ƒæ€»ç»“",
            summaries["releases"] if summaries["releases"] else "è¯¥æ—¶é—´æ®µå†…æ— å‘å¸ƒè®°å½•",
            "",
            "### ğŸ“¢ ç¤¾åŒºæ´»åŠ¨æ€»ç»“ï¼ˆIssues/PRï¼‰",
            summaries["issues_prs"] if summaries["issues_prs"] else "è¯¥æ—¶é—´æ®µå†…æ— Issueså’ŒPRæ´»åŠ¨",
            "",
            "---",
            "",
        ])

        # åŸå§‹æ•°æ®é¢„è§ˆ
        md.append("## ğŸ” åŸå§‹æ•°æ®é¢„è§ˆ")
        
        # å‘å¸ƒé¢„è§ˆ
        if raw_data["releases"]:
            md.extend([
                "### æœ€æ–°å‘å¸ƒ",
                "| ç‰ˆæœ¬ | å‘å¸ƒæ—¶é—´ | æ ‡é¢˜ |",
                "|------|----------|------|",
            ])
            for r in raw_data["releases"][:5]:
                publish_time = self._parse_iso_datetime(r.get('published_at')).strftime("%Y-%m-%d")
                md.append(f"| {r.get('tag_name', 'æœªçŸ¥')} | {publish_time} | {r.get('name', '')[:50]} |")
            md.append("")
        
        # PRé¢„è§ˆ
        if raw_data["pull_requests"]:
            md.extend([
                "### æœ€è¿‘PR",
                "| ç¼–å· | çŠ¶æ€ | æ ‡é¢˜ | åˆ›å»ºè€… |",
                "|------|------|------|--------|",
            ])
            for pr in raw_data["pull_requests"][:5]:
                md.append(f"| #{pr.get('number')} | {pr.get('state')} | {pr.get('title', '')[:50]} | {pr.get('user', {}).get('login')} |")
            md.append("")

        # Issuesé¢„è§ˆ
        if raw_data["issues"]:
            md.extend([
                "### æœ€è¿‘Issues",
                "| ç¼–å· | çŠ¶æ€ | æ ‡é¢˜ | åˆ›å»ºè€… |",
                "|------|------|------|--------|",
            ])
            for issue in raw_data["issues"][:5]:
                md.append(f"| #{issue.get('number')} | {issue.get('state')} | {issue.get('title', '')[:50]} | {issue.get('user', {}).get('login')} |")
            md.append("")

        return "\n".join(md)
    