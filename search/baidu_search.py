import requests
import json
import time
import logging
from typing import Dict, Optional, List, Union
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class BaiduQianfanSearch:
    """
    ç™¾åº¦åƒå¸†æœç´¢APIå°è£…ç±»
    ä½¿ç”¨ç™¾åº¦åƒå¸†å¹³å°æä¾›çš„æœç´¢APIè¿›è¡Œæœç´¢
    
    æ³¨æ„ï¼šéœ€è¦å…ˆåœ¨ç™¾åº¦åƒå¸†å¹³å°è·å–APIå¯†é’¥
    """
    
    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–ç™¾åº¦åƒå¸†æœç´¢API
        
        å‚æ•°:
            api_key: ç™¾åº¦åƒå¸†å¹³å°çš„APIå¯†é’¥
        """
        self.api_key = api_key
        self.base_url = "https://qianfan.baidubce.com/v2/ai_search/web_search"
        self.headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {api_key}'
        }
        
        # APIè°ƒç”¨ç»Ÿè®¡
        self.call_count = 0
        self.last_call_time = 0
        
    def search(self, 
              query: str, 
              edition: str = "standard",
              search_source: str = "baidu_search_v2",
              search_recency_filter: str = "week",
              timeout: int = 30) -> Optional[Dict]:
        """
        æ‰§è¡Œç™¾åº¦åƒå¸†æœç´¢
        
        å‚æ•°:
            query: æœç´¢å…³é”®è¯
            edition: APIç‰ˆæœ¬ï¼Œé»˜è®¤ä¸º"standard"
            search_source: æœç´¢æ¥æºï¼Œé»˜è®¤ä¸º"baidu_search_v2"
            search_recency_filter: æ—¶é—´è¿‡æ»¤ï¼Œå¯é€‰å€¼: "day", "week", "month", "year"
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤ä¸º30ç§’
        
        è¿”å›:
            åŒ…å«æœç´¢ç»“æœçš„å­—å…¸ï¼Œæˆ–Noneè¡¨ç¤ºå¤±è´¥
        """
        try:
            # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ
            if not self.api_key:
                print("âŒ APIå¯†é’¥ä¸èƒ½ä¸ºç©º")
                return None
            
            # æ£€æŸ¥è¯·æ±‚é¢‘ç‡ï¼ˆç®€å•é™æµï¼‰
            current_time = time.time()
            if current_time - self.last_call_time < 1:  # é™åˆ¶1ç§’å†…æœ€å¤š1æ¬¡è¯·æ±‚
                time.sleep(1 - (current_time - self.last_call_time))
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            payload = {
                "messages": [
                    {
                        "role": "user",
                        "content": query
                    }
                ],
                "edition": edition,
                "search_source": search_source,
                "search_recency_filter": search_recency_filter
            }
            
            print(f"ğŸ” æ­£åœ¨ä½¿ç”¨ç™¾åº¦åƒå¸†æœç´¢: {query}")
            print(f"ğŸ“‹ æœç´¢å‚æ•°: {json.dumps(payload, ensure_ascii=False, indent=2)}")
            
            # å‘é€è¯·æ±‚
            response = requests.post(
                self.base_url,
                headers=self.headers,
                data=json.dumps(payload, ensure_ascii=False).encode("utf-8"),
                timeout=timeout
            )
            
            # æ›´æ–°è°ƒç”¨ç»Ÿè®¡
            self.call_count += 1
            self.last_call_time = time.time()
            
            # æ£€æŸ¥å“åº”çŠ¶æ€ç 
            response.raise_for_status()
            
            # è®¾ç½®å“åº”å“åº”ç¼–ç 
            response.encoding = "utf-8"
            
            # è§£æJSONå“åº”
            result = response.json()
            
            print(f"âœ… æœç´¢æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            return result
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ æœç´¢è¯·æ±‚å¤±è´¥: {e}")
            if hasattr(e, 'response') and e.response:
                try:
                    error_detail = e.response.json()
                    print(f"âŒ é”™è¯¯è¯¦æƒ…: {json.dumps(error_detail, ensure_ascii=False, indent=2)}")
                except:
                    print(f"âŒ é”™è¯¯å“åº”: {e.response.text[:500]}")
            return None
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            print(f"âŒ å“åº”å†…å®¹: {response.text[:500]}")
            return None
        except Exception as e:
            print(f"âŒ æœç´¢è¿‡ç¨‹å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def fetch_page_content(self, url: str, timeout: int = 10) -> Optional[str]:
        """
        é€šè¿‡URLè·å–é¡µé¢é™æ€å†…å®¹ï¼Œæå–å…³é”®æ–‡æœ¬
        
        å‚æ•°:
            url: è¦è·å–çš„é¡µé¢URL
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤ä¸º10ç§’
        
        è¿”å›:
            æå–çš„é¡µé¢æ–‡æœ¬å†…å®¹ï¼Œæˆ–Noneè¡¨ç¤ºå¤±è´¥
        """
        try:
            # éªŒè¯URLæ ¼å¼
            parsed_url = urlparse(url)
            if not parsed_url.scheme or not parsed_url.netloc:
                print(f"âŒ æ— æ•ˆçš„URLæ ¼å¼: {url}")
                return None
            
            # åˆ›å»ºä¼šè¯å¹¶è®¾ç½®é‡è¯•ç­–ç•¥
            session = requests.Session()
            retry_strategy = Retry(
                total=3,
                backoff_factor=1,
                status_forcelist=[429, 500, 502, 503, 504]
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            
            # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.5,en;q=0.3',
                'Connection': 'keep-alive'
            }
            
            print(f"ğŸŒ æ­£åœ¨é¡µé¢å†…å®¹: {url}")
            
            # å‘é€GETè¯·æ±‚
            response = session.get(
                url,
                headers=headers,
                timeout=timeout,
                allow_redirects=True
            )
            response.raise_for_status()
            
            # è®¾ç½®æ­£ç¡®çš„ç¼–ç 
            response.encoding = response.apparent_encoding
            
            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–é¡µé¢æ ‡é¢˜
            title = soup.title.string.strip() if soup.title else "æ— æ ‡é¢˜"
            
            # æå–æ­£æ–‡å†…å®¹
            # å°è¯•æå–å¸¸è§çš„æ­£æ–‡æ ‡ç­¾
            content_tags = ['div', 'article', 'main', 'section']
            content = ""
            
            for tag in content_tags:
                elements = soup.find_all(tag)
                for elem in elements:
                    # è¿‡æ»¤æ‰å¯èƒ½ä¸æ˜¯æ­£æ–‡çš„å†…å®¹
                    if not any(cls in elem.get('class', []) for cls in ['nav', 'menu', 'sidebar', 'footer', 'header', 'advertisement']):
                        paragraphs = elem.find_all('p')
                        if paragraphs:
                            content += '\n'.join([p.get_text(strip=True) for p in paragraphs])
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„å†…å®¹ï¼Œå°è¯•æå–æ‰€æœ‰æ–‡æœ¬
            if not content or len(content) < 100:
                # æå–æ‰€æœ‰æ–‡æœ¬ï¼Œä½†è¿‡æ»¤æ‰è¿‡çŸ­çš„å†…å®¹
                all_text = soup.get_text(separator='\n', strip=True)
                # æŒ‰æ®µè½åˆ†å‰²å¹¶è¿‡æ»¤
                paragraphs = [p for p in all_text.split('\n') if len(p) > 50]
                content = '\n\n'.join(paragraphs[:10])  # å–å‰10ä¸ªè¾ƒé•¿çš„æ®µè½
            
            # æ¸…ç†å†…å®¹
            content = content.strip()
            
            # å¦‚æœå†…å®¹ä»ç„¶å¤ªçŸ­ï¼Œä½¿ç”¨å…ƒæè¿°
            if not content or len(content) < 50:
                meta_description = soup.find('meta', attrs={'name': 'description'})
                if meta_description and meta_description.get('content'):
                    content = meta_description['content']
            
            # è¾“å‡ºåˆ°æ—¥å¿—
            if content:
                print(f"ğŸ“„ æå–é¡µé¢å†…å®¹æˆåŠŸ: {title}")
                print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"ğŸ” å†…å®¹é¢„è§ˆ: {content[:200]}..." if len(content) > 200 else f"ğŸ” å†…å®¹: {content}")
                print("-" * 80)
            else:
                print(f"âš ï¸  æœªèƒ½ä»é¡µé¢æå–æœ‰æ•ˆå†…å®¹: {url}")
            
            return content
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ è·å–é¡µé¢å†…å®¹å¤±è´¥: {e}")
            if hasattr(e, 'response') and e.response:
                print(f"âŒ çŠ¶æ€ç : {e.response.status_code}")
            return None
        except Exception as e:
            print(f"âŒ è§£æé¡µé¢å†…å®¹å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return None

# å…¨å±€å˜é‡å­˜å‚¨æœç´¢å™¨å®ä¾‹
searcher = None

def initialize_searcher(api_key: str) -> str:
    """åˆå§‹åŒ–æœç´¢å™¨"""
    global searcher
    try:
        if api_key:
            searcher = BaiduQianfanSearch(api_key)
            return "âœ… æœç´¢å™¨åˆå§‹åŒ–æˆåŠŸï¼"
        else:
            searcher = None
            return "âš ï¸ APIå¯†é’¥ä¸ºç©ºï¼Œè¯·è¾“å…¥æœ‰æ•ˆçš„APIå¯†é’¥ã€‚"
    except Exception as e:
        searcher = None
        return f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}"

def search_function(query: str) -> List[Dict]:
    """ä½¿ç”¨ç™¾åº¦åƒå¸†APIè¿›è¡Œæœç´¢"""
    global searcher
    
    # æ£€æŸ¥æœç´¢å™¨æ˜¯å¦å·²åˆå§‹åŒ–
    if not searcher:
        return []
    
    try:
        # è°ƒç”¨ç™¾åº¦åƒå¸†APIè¿›è¡Œæœç´¢
        result = searcher.search(query)
        
        if not result:
            return []
        
        # è§£ææœç´¢ç»“æœ
        search_results = []
        
        # æ£€æŸ¥å“åº”ç»“æ„
        if "references" in result:
            # é¦–å…ˆå°è¯•ä» references æ•°ç»„è·å–ç»“æœ
                for item in result["references"]:
                    # æå–æ ‡é¢˜ã€å†…å®¹å’ŒURL
                    title = item.get("title", "æ— æ ‡é¢˜")
                    content = item.get("content", item.get("snippet", "æ— å†…å®¹"))
                    url = item.get("url", "")
                    
                    # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œå°è¯•ä»å…¶ä»–å­—æ®µè·å–
                    if len(str(content)) < 50:
                        content = item.get("snippet", "æ— å†…å®¹")
                    
                    search_results.append({
                        "title": title,
                        "content": content,
                        "url": url
                    })       
        print(f"ğŸ“Š è§£æåˆ° {len(search_results)} ä¸ªæœç´¢ç»“æœ")
        
        # æŠ“å–æ¯ä¸ªæœç´¢ç»“æœçš„é¡µé¢å†…å®¹
        print("\n" + "="*60)
        print("å¼€å§‹æŠ“å–é¡µé¢å†…å®¹...")
        print("="*60)
        
        for i, result in enumerate(search_results):
            url = result.get("url")
            if url and url.startswith(("http://", "https://")):
                try:
                    print(f"\nğŸ” æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(search_results)} ä¸ªç»“æœ:")
                    print(f"   URL: {url}")
                    
                    # è°ƒç”¨fetch_page_contentæ–¹æ³•è·å–é¡µé¢å†…å®¹
                    page_content = searcher.fetch_page_content(url)
                    
                    # å¦‚æœè·å–åˆ°å†…å®¹ï¼Œæ›´æ–°ç»“æœä¸­çš„contentå­—æ®µ
                    if page_content and len(page_content) > len(result["content"]):
                        # ä¿ç•™åŸå§‹æ‘˜è¦çš„å‰200ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦ï¼Œå®Œæ•´å†…å®¹å­˜å‚¨åœ¨full_contentä¸­
                        result["full_content"] = page_content
                        # æ›´æ–°contentä¸ºæ›´è¯¦ç»†çš„æ‘˜è¦
                        if len(page_content) > 300:
                            result["content"] = page_content[:300] + "..."
                        
                except Exception as e:
                    print(f"âŒ å¤„ç†URL {url} æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
            else:
                print(f"\nâš ï¸  æ— æ•ˆçš„URL: {url}")
        
        print("\n" + "="*60)
        print("é¡µé¢å†…å®¹æŠ“å–å®Œæˆ")
        print("="*60)
        
        return search_results
        
    except Exception as e:
        print(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return []
